# HadoopStudy
데이터 엔지니어링 스터디
졸작 만들면서 공부하기

-> Google cloud 에서도 spark를 돌릴 수 있다 
-> Google cloud + spark + firebase + androidStudio
-> scala 문법 공부

big data engineering

spark sql & spark ML 


1. 하둡과 아파치 스파크의 역할

하듑 = 분산 데이터 인프라스트럭처, 대량의 데이터를 서버 클러스터 내 복수의 노드들에 분산시키는 역할, 스토리지
스파크 = 분산형 데이터 컬렉션 상부에서 동작하는 데이터 프로세싱 툴, 스토리지 X


2. 하둡과 아파치는 상호 독립적, 상호 보완적이다

하듑 = 분산형 파일 시스템인 HDFS + 분산형 파일 처리 시스템인 맵리듀스
스파크 =  굳이 HDFS가 아니더라도 여타 클라우드 기반의 데이터 플랫폼(대표적으로 AWS)들과 융합가능
그러나 둘은 함께 할 때 가장 좋은 궁합을 보인다.


3. 속도

스파크 > 하듑=맵리듀스
하듑 = 단계졀 데이터 처리 절차를 따름, 
"먼저 클러스터에서 데이터를 읽고(클라이언트->네임노드->클라이언트->데이터 노드-> 마스터(Job Tracker)), 태스크 단위로 쪼개어 Tasketracker(worker)에 배정한 뒤 Map 단계를 수행한 후, 중간 결과물을 로컬 디스크에 저장을 한다. 그리고 그 결과물을 다시 combine, partioning을 거쳐 나온 2차 중간 결과물을 디스크에 분할 저장한다. 그리고 최종적으로 shuffling을 통해 reduce 작업에 할당된 후, reduce 작업을 거쳐 최종적으로 나온 결과물이 HDFS에 저장된다"
처리-> 저장 -> 처리 -> 저장
스파크 = 전체 데이터셋을 한번에 다룸, 모든 데이터 운영을 메모리 내에서 실시간에 가깝게 처리할 수 있다(인메모리). 데이터를 읽고, 처리 분석을 거친 결과물을 클러스터에 입력하는 전 과정이 동시에 진행


4. 그렇다면 스파크만 쓰면 되지 왜 하둡?

데이터 운영 및 리포팅 요구 대부분이 정적인 것이고, 배치 모드의 프로세싱을 기다릴 수 있다면, 굳이 스파크를 쓰지 않고 맵리듀스 프로세싱 방식을 채택해도 무방하다. 스파크가 필수적인 비즈니스는 공장과 같이 센서에서 실시간으로 수집되는 스티리밍 데이터를 처리하거나, 머신러닝 알고리즘과 같이 애플리케이션이 복합적인 운영을 필요로 하는 경우이다. 구체적은 사례로, 마케팅 캠페인, 상품 추천, 사이버 보안 분석, 기계 로그 모니터링 등의 애플리케이션 작업에 스파크가 탁월한 성능을 발휘할 수 있다.

 

5. 고장 감내성 측면에서는?

하둡의 경우 프로세싱 절차마다의 기록을 디스크에 기록하여 failover를 하는 방식이다. 스파크의 경우에는 탄력적 분산형 데이터넷(RDD, Resilient Distributed Dataset)이라는 형태로 데이터 오브젝트들을 데이터 클러스터 전반에 분산시킴으로서 탄력성을 보장한다. RDD 오브젝트들은 매모리 내 또는 디스크에 저장할 수 있으며 사고나 고장이 나더라도 완벽하게 복구할수 있는 기술이다.
